{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Multi-Output Regression Module\n",
    "=============================\n",
    "\n",
    "This is an example notebook detailing the functions currently contained in this module for Multi-Output (or Multi-Task) Regression. These include:\n",
    "\n",
    "1. Network structure discovery in continuous domains for modeling relations and feature selection.\n",
    "2. Combine several sampled networks to create an empirical approximation of the posterior distribution over network structures. This allows for a more robust estimation, especially when few samples are available.\n",
    "3. Use of the learned structure to fit a Multi-Task Regression model that can perform regression over several target variables simultaneously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.random import RandomState\n",
    "\n",
    "rng = RandomState(1802)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step is to load some data. In this example we will generate some random data from a Gaussian Network with fixed, dummy parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from core.gaussian import sample_from_gn\n",
    "from structure.graph_generation import random_mbc\n",
    "from structure.graphs import plot_digraph\n",
    "\n",
    "# We use 15 variables, 10 features amd 5 targets\n",
    "n_variables = 15\n",
    "n_features, n_targets = 10, 5\n",
    "variables = list(range(n_variables))\n",
    "\n",
    "# Data generation parameters\n",
    "n_samples = 200\n",
    "test_samples = 100\n",
    "\n",
    "# the mean, variance, and weight of the interaction\n",
    "gen_mean = np.zeros(n_variables)\n",
    "gen_var = np.zeros(n_variables) + 0.2\n",
    "gen_weight = 2\n",
    "\n",
    "# sample a random structure with a given fan in to obtain a sparse graph\n",
    "fan_in = 5\n",
    "graph = random_mbc(n_features, n_targets, rng=rng, fan_in=fan_in)\n",
    "\n",
    "# Generate the data from the parameters\n",
    "data = sample_from_gn(graph, gen_mean, gen_var, gen_weight, n_samples + test_samples, rng)\n",
    "\n",
    "training_data, test_data = data[:n_samples], data[n_samples:]\n",
    "\n",
    "# Plot the digraph\n",
    "plot_digraph(graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To obtain samples of DAG structures we will use MCMC sampling. To this end we need a state sapce, a proposal that defines the neighborhood of the Directed Acyclic Graphs (DAGs) in this space and score function to determine how good these structures are given the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from metrics.score import BGe\n",
    "from structure.graphs import plot_digraph\n",
    "from mcmc.graphs.sampler import MHStructureSampler\n",
    "from mcmc.graphs.proposal import MBCProposal, basic_move, rev_move, nbhr_move\n",
    "\n",
    "# First we define the proposal. We will use three moves: Basica addition and removal of edges,\n",
    "# the REV move and NBHR move, with given probabilities of being used\n",
    "moves = [basic_move, rev_move, nbhr_move]\n",
    "move_probs = [13/15, 1/15, 1/15]\n",
    "\n",
    "# The MBCProposal already incorportates the necessary constraints on the graphs. The BGe is a\n",
    "# metric specifically designed for scoring Gaussian Networks and it is the one implemented here\n",
    "proposal = MBCProposal(moves, move_prob=move_probs, score=BGe, fan_in=5, random_state=rng)\n",
    "\n",
    "# Initialize a structure sampler using Metropolis-Hastings with the above defined proposal.\n",
    "sampler = MHStructureSampler(\n",
    "    proposal=proposal, n_steps=10000, sample_freq=100, burn_in=5000, verbose=True, rng=rng\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point we can collect samples by simply running the sampler with our training data: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X, y = training_data[:, :n_features], training_data[:, n_features:]\n",
    "\n",
    "# If return scores is true, the method will return a tuple of networks and \n",
    "# their respective scores according to the used metric (in this case BGe)\n",
    "trace = sampler.generate_samples((X, y), return_scores=True)\n",
    "samples, scores = trace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With these samples we can create an empiracal distribution over networks and check the (conditional) probability of edges:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from mcmc.graphs.sampler import DAGDistribution\n",
    "dist = DAGDistribution(samples)\n",
    "\n",
    "print('The marginal probability of edge 0 --> 1 is {0}'.format(dist.edge_prob((0, 1))))\n",
    "print('The marginal probability of edge 0 --> 1 given edge 2 --> 3 is present in the graph is {0}'\n",
    "      .format(dist.edge_conditional_prob((0, 1), (2, 3)))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can train a Multi-output regression model called Multi-ouput Gaussian Network Regressor Ensamble using these sampled netowkrs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from model.mgnr import MGNREnsemble\n",
    "\n",
    "# We initialize the model with the number of networks we want to combine and the optimizer we want to use in case\n",
    "# samples are not passed as an argument during the fitting procedure (in this case we use the computed samples)\n",
    "model = MGNREnsemble(k=100, optimizer=sampler, rng=rng, verbose=True).fit(X, y, samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model can be tested using the test samples we generated above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_test, y_test = test_data[:, :n_features], test_data[:, n_features:]\n",
    "\n",
    "sq_error = 0\n",
    "for x, y in zip(X_test, y_test):\n",
    "    predicted = model.predict(X).reshape(1, -1)\n",
    "\n",
    "    sq_error += (y - predicted) ** 2\n",
    "\n",
    "rmse = np.sqrt(sq_error / X_test.shape[0])\n",
    "print('The RMSE error was: {:.2f}'.format(rmse))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
